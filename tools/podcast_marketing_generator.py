#!/usr/bin/env python3
"""ç‹¬ç«‹æ’­å®¢å†…å®¹ç”Ÿæˆå™¨

è¿ç§»è‡ª personal/Translate é¡¹ç›®ï¼Œè°ƒæ•´ä¸ºä½¿ç”¨ index-tts ä¸­çš„ OpenAI å…¼å®¹é…ç½®ã€‚
"""

import argparse
import json
import logging
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from translate_openai_cli import (
    DEFAULT_ENV_PATH,
    build_endpoint,
    load_env_file,
    post_chat_completions,
)

# ç‹¬ç«‹æ’­å®¢ç”Ÿæˆå™¨çš„æç¤ºè¯å®šä¹‰
PODCAST_CONTENT_GENERATION_ROLE = "You are a professional podcast content strategist and marketing expert specializing in creating compelling podcast content. You excel at extracting the most engaging moments from conversations and crafting irresistible marketing materials."

# ç‹¬ç«‹æ’­å®¢ç”Ÿæˆå™¨çš„é»˜è®¤æ¨¡å‹é…ç½®ç”±å‘½ä»¤è¡Œå‚æ•°/env æ§åˆ¶


@dataclass
class LLMConfig:
    base_url: str
    api_key: str
    model: str
    temperature: float = 0.2
    top_p: float = 1.0
    max_tokens: Optional[int] = None
    timeout: int = 300
    response_format: Optional[str] = "json_object"


class SubtitleProcessor:
    """å­—å¹•æ–‡ä»¶å¤„ç†å™¨"""
    
    @staticmethod
    def load_subtitle_json(file_path: str) -> Dict[str, Any]:
        """åŠ è½½JSONå­—å¹•æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logging.error(f"âŒ åŠ è½½å­—å¹•æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    @staticmethod
    def extract_text_with_timestamps(subtitle_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»å­—å¹•æ•°æ®ä¸­æå–æ–‡æœ¬å’Œæ—¶é—´æˆ³

        æ”¯æŒå¤šç§å­—å¹•æ ¼å¼ï¼š
        - segments: [{"start": 0.0, "end": 5.0, "text": "..."}]
        - transcription: {"segments": [...]}
        - results: [{"start": 0.0, "end": 5.0, "text": "..."}]
        - ç›´æ¥æ•°ç»„æ ¼å¼
        """
        segments = []
        raw_segments = None

        # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
        if isinstance(subtitle_data, dict):
            # å°è¯•å¸¸è§çš„é”®å
            possible_keys = [
                "segments",
                "results",
                "transcripts",
                "items",
                "subtitles",
            ]
            for key in possible_keys:
                if key in subtitle_data and isinstance(subtitle_data[key], list):
                    raw_segments = subtitle_data[key]
                    logging.info(f"ğŸ” æ‰¾åˆ°æ•°æ®åœ¨é”® '{key}' ä¸‹ï¼ŒåŒ…å« {len(raw_segments)} ä¸ªå…ƒç´ ")
                    break

            # å°è¯•åµŒå¥—ç»“æ„
            if raw_segments is None:
                nested_keys = [
                    ("transcription", "segments"),
                    ("data", "segments"),
                    ("result", "segments"),
                    ("response", "segments")
                ]
                for parent_key, child_key in nested_keys:
                    if (parent_key in subtitle_data and
                        isinstance(subtitle_data[parent_key], dict) and
                        child_key in subtitle_data[parent_key] and
                        isinstance(subtitle_data[parent_key][child_key], list)):
                        raw_segments = subtitle_data[parent_key][child_key]
                        logging.info(f"ğŸ” æ‰¾åˆ°æ•°æ®åœ¨åµŒå¥—ç»“æ„ '{parent_key}.{child_key}' ä¸‹ï¼ŒåŒ…å« {len(raw_segments)} ä¸ªå…ƒç´ ")
                        break
        elif isinstance(subtitle_data, list):
            raw_segments = subtitle_data
            logging.info(f"ğŸ” æ•°æ®ä¸ºç›´æ¥æ•°ç»„æ ¼å¼ï¼ŒåŒ…å« {len(raw_segments)} ä¸ªå…ƒç´ ")

        if raw_segments is None:
            logging.error("âŒ æ— æ³•åœ¨JSONä¸­æ‰¾åˆ°å­—å¹•æ•°æ®")
            return segments

        # å¤„ç†æ¯ä¸ªæ®µè½
        for i, segment in enumerate(raw_segments):
            if not isinstance(segment, dict):
                logging.warning(f"âš ï¸ è·³è¿‡éå­—å…¸æ ¼å¼çš„æ®µè½ {i}: {type(segment)}")
                continue

            # å°è¯•ä¸åŒçš„å­—æ®µå
            text_fields = ["text", "content", "transcript", "sentence"]
            start_fields = ["start", "start_time", "time_begin", "begin", "from"]
            end_fields = ["end", "end_time", "time_end", "finish", "to"]

            text = None
            start_time = None
            end_time = None

            # æŸ¥æ‰¾æ–‡æœ¬å­—æ®µ
            for field in text_fields:
                if field in segment and segment[field]:
                    text = str(segment[field]).strip()
                    break

            # æŸ¥æ‰¾å¼€å§‹æ—¶é—´å­—æ®µ
            for field in start_fields:
                if field in segment and segment[field] is not None:
                    try:
                        time_value = float(segment[field])
                        # å¦‚æœæ—¶é—´å€¼å¾ˆå¤§ï¼ˆ>1000ï¼‰ï¼Œå¯èƒ½æ˜¯æ¯«ç§’ï¼Œéœ€è¦è½¬æ¢ä¸ºç§’
                        if time_value > 1000:
                            start_time = time_value / 1000.0
                        else:
                            start_time = time_value
                        break
                    except (ValueError, TypeError):
                        continue

            # æŸ¥æ‰¾ç»“æŸæ—¶é—´å­—æ®µ
            for field in end_fields:
                if field in segment and segment[field] is not None:
                    try:
                        time_value = float(segment[field])
                        # å¦‚æœæ—¶é—´å€¼å¾ˆå¤§ï¼ˆ>1000ï¼‰ï¼Œå¯èƒ½æ˜¯æ¯«ç§’ï¼Œéœ€è¦è½¬æ¢ä¸ºç§’
                        if time_value > 1000:
                            end_time = time_value / 1000.0
                        else:
                            end_time = time_value
                        break
                    except (ValueError, TypeError):
                        continue

            # å¦‚æœæ‰¾åˆ°äº†æ–‡æœ¬å’Œå¼€å§‹æ—¶é—´ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if text and start_time is not None:
                segments.append({
                    "text": text,
                    "start": start_time,
                    "end": end_time if end_time is not None else start_time,
                    "timestamp": SubtitleProcessor.format_timestamp(start_time)
                })
            else:
                logging.debug(f"âš ï¸ è·³è¿‡æ®µè½ {i}ï¼Œç¼ºå°‘å¿…è¦å­—æ®µ - text: {bool(text)}, start: {start_time is not None}")

        return segments
    
    @staticmethod
    def format_timestamp(seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸ºæ—¶é—´æˆ³æ ¼å¼ (MM:SS)"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes:02d}:{secs:02d}"
    
    @staticmethod
    def combine_segments_to_text(segments: List[Dict[str, Any]]) -> str:
        """å°†åˆ†æ®µæ–‡æœ¬åˆå¹¶ä¸ºå®Œæ•´æ–‡æœ¬"""
        return " ".join([seg["text"] for seg in segments if seg["text"]])


class EnhancedPodcastContentGenerator:
    """å¢å¼ºç‰ˆæ’­å®¢å†…å®¹ç”Ÿæˆå™¨ï¼Œæ”¯æŒæ—¶é—´æˆ³"""

    def __init__(self, llm_config: LLMConfig):
        self.llm_config = llm_config
        self.endpoint = build_endpoint(llm_config.base_url)

        # åˆå§‹åŒ– token ç»Ÿè®¡
        self.total_tokens = {
            "prompt": 0,
            "completion": 0,
            "total": 0,
        }

    def _update_token_counts(self, token_counts: Dict[str, int]):
        """æ›´æ–°ç´¯è®¡tokenè®¡æ•°"""
        for key in self.total_tokens:
            if key == "completion":
                # æ˜ å°„completion_tokensåˆ°completion
                self.total_tokens[key] += token_counts.get("completion_tokens", 0)
            else:
                self.total_tokens[key] += token_counts.get(key, 0)

    def _display_usage_summary(self):
        """æ˜¾ç¤ºtokenä½¿ç”¨æƒ…å†µå’Œæˆæœ¬æ€»ç»“"""
        logging.info("=" * 60)
        logging.info("ğŸ“Š æ’­å®¢å†…å®¹ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯")
        logging.info("=" * 60)
        logging.info(f"ğŸ”¢ Tokenä½¿ç”¨æƒ…å†µ:")
        logging.info(f"   - è¾“å…¥Token: {self.total_tokens['prompt']:,}")
        logging.info(f"   - è¾“å‡ºToken: {self.total_tokens['completion']:,}")
        logging.info(f"   - æ€»Token: {self.total_tokens['total']:,}")
        logging.info("ğŸ’° ä¼°ç®—æˆæœ¬: æš‚æœªè®¡ç®—ï¼ˆå¯ç»“åˆå®é™…å®šä»·æ‰‹åŠ¨ä¼°ç®—ï¼‰")
        logging.info("=" * 60)
        
    def generate_content_from_subtitle(self, subtitle_file: Path, output_dir: Path) -> Optional[Path]:
        """ä»å­—å¹•æ–‡ä»¶ç”Ÿæˆæ’­å®¢å†…å®¹"""

        # 1. åŠ è½½å’Œå¤„ç†å­—å¹•æ–‡ä»¶
        logging.info(f"ğŸ“ åŠ è½½å­—å¹•æ–‡ä»¶: {subtitle_file}")
        subtitle_data = SubtitleProcessor.load_subtitle_json(str(subtitle_file))

        # è°ƒè¯•ï¼šæ˜¾ç¤ºJSONæ–‡ä»¶çš„ç»“æ„
        logging.info(f"ğŸ” JSONæ–‡ä»¶ç»“æ„è°ƒè¯•:")
        logging.info(f"   - é¡¶å±‚é”®: {list(subtitle_data.keys()) if isinstance(subtitle_data, dict) else 'ä¸æ˜¯å­—å…¸ç±»å‹'}")
        if isinstance(subtitle_data, dict):
            for key, value in subtitle_data.items():
                if isinstance(value, list):
                    logging.info(f"   - {key}: åˆ—è¡¨ï¼Œé•¿åº¦ {len(value)}")
                    if len(value) > 0:
                        logging.info(f"     é¦–ä¸ªå…ƒç´ ç±»å‹: {type(value[0])}")
                        if isinstance(value[0], dict):
                            logging.info(f"     é¦–ä¸ªå…ƒç´ é”®: {list(value[0].keys())}")
                elif isinstance(value, dict):
                    logging.info(f"   - {key}: å­—å…¸ï¼Œé”®: {list(value.keys())}")
                else:
                    logging.info(f"   - {key}: {type(value)}")

        segments = SubtitleProcessor.extract_text_with_timestamps(subtitle_data)

        logging.info(f"âœ… æˆåŠŸæå– {len(segments)} ä¸ªæ–‡æœ¬æ®µè½")

        # å¦‚æœæ²¡æœ‰æå–åˆ°ä»»ä½•æ®µè½ï¼Œåœæ­¢å¤„ç†
        if len(segments) == 0:
            logging.error("âŒ æœªèƒ½ä»å­—å¹•æ–‡ä»¶ä¸­æå–åˆ°ä»»ä½•æ–‡æœ¬æ®µè½ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")
            logging.error("ğŸ’¡ æ”¯æŒçš„æ ¼å¼ç¤ºä¾‹:")
            logging.error("   1. {\"segments\": [{\"start\": 0.0, \"end\": 5.0, \"text\": \"...\"}]}")
            logging.error("   2. {\"transcription\": {\"segments\": [...]}}")
            logging.error("   3. [{\"start\": 0.0, \"end\": 5.0, \"text\": \"...\"}]")
            return None

        # è®¡ç®—éŸ³é¢‘æ€»æ—¶é•¿
        total_duration = 0
        if segments:
            total_duration = max(seg.get('end', seg.get('start', 0)) for seg in segments)

        # æ ¼å¼åŒ–æ€»æ—¶é•¿
        total_duration_formatted = self._format_duration(total_duration)

        logging.info(f"ğŸ•’ éŸ³é¢‘æ€»æ—¶é•¿: {total_duration_formatted} ({total_duration:.1f}ç§’)")

        # 2. ç”Ÿæˆæ’­å®¢å†…å®¹
        logging.info("ğŸ¯ å¼€å§‹ç”Ÿæˆæ’­å®¢è¥é”€å†…å®¹...")
        enhanced_prompt = self._create_enhanced_prompt(segments, total_duration, total_duration_formatted)
        
        # è°ƒç”¨LLMç”Ÿæˆå†…å®¹
        response = self._call_llm_for_content_generation(enhanced_prompt)
        
        if not response:
            logging.error("âŒ æ’­å®¢å†…å®¹ç”Ÿæˆå¤±è´¥")
            return None
        
        # 3. ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"podcast_content_{timestamp}.md"

        # åˆ›å»ºç»¼åˆå†…å®¹æ–‡æ¡£ï¼ˆåŒ…å«çº¯æ–‡æœ¬Show Notesï¼‰
        comprehensive_content = self._create_comprehensive_content_with_plain_show_notes(response)
        
        with output_file.open('w', encoding='utf-8') as f:
            f.write(comprehensive_content)

        logging.info(f"âœ… æ’­å®¢å†…å®¹å·²ä¿å­˜åˆ°: {output_file}")

        # éªŒè¯ç”Ÿæˆçš„æ—¶é—´è½´å®Œæ•´æ€§
        if response and 'show_notes' in response and 'timeline' in response['show_notes']:
            self._validate_timeline_completeness(response['show_notes']['timeline'], total_duration, total_duration_formatted)

        # æ˜¾ç¤ºtokenä½¿ç”¨æƒ…å†µå’Œæˆæœ¬ç»Ÿè®¡
        self._display_usage_summary()

        return output_file
    
    def _create_enhanced_prompt(self, segments: List[Dict[str, Any]], total_duration: float, total_duration_formatted: str) -> str:
        """åˆ›å»ºå¢å¼ºçš„æç¤ºï¼ŒåŒ…å«æ—¶é—´æˆ³ä¿¡æ¯"""

        # åˆ›å»ºä¸“ä¸šçº§Show Notesæç¤ºè¯
        professional_shownotes_prompt = self._create_professional_shownotes_prompt(segments, total_duration, total_duration_formatted)

        return professional_shownotes_prompt

    def _format_duration(self, seconds: float) -> str:
        """å°†ç§’æ•°è½¬æ¢ä¸ºæ—¶é•¿æ ¼å¼ (HH:MM:SS æˆ– MM:SS)"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)

        if hours > 0:
            return f"{hours:02d}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes:02d}:{secs:02d}"

    def _validate_timeline_completeness(self, timeline: List[Dict[str, Any]], total_duration: float, total_duration_formatted: str):
        """éªŒè¯ç”Ÿæˆçš„æ—¶é—´è½´æ˜¯å¦è¦†ç›–äº†å®Œæ•´çš„éŸ³é¢‘æ—¶é•¿"""
        if not timeline:
            logging.warning("âš ï¸ æ—¶é—´è½´ä¸ºç©º")
            return

        # è§£ææ—¶é—´è½´ä¸­çš„æœ€åä¸€ä¸ªæ—¶é—´ç‚¹
        last_timeline_entry = timeline[-1]
        last_time_str = last_timeline_entry.get('time', '00:00')

        try:
            # è§£ææ—¶é—´å­—ç¬¦ä¸² (MM:SS æˆ– HH:MM:SS)
            time_parts = last_time_str.split(':')
            if len(time_parts) == 2:  # MM:SS
                last_time_seconds = int(time_parts[0]) * 60 + int(time_parts[1])
            elif len(time_parts) == 3:  # HH:MM:SS
                last_time_seconds = int(time_parts[0]) * 3600 + int(time_parts[1]) * 60 + int(time_parts[2])
            else:
                logging.warning(f"âš ï¸ æ— æ³•è§£ææ—¶é—´æ ¼å¼: {last_time_str}")
                return

            # è®¡ç®—è¦†ç›–ç‡
            coverage_percentage = (last_time_seconds / total_duration) * 100 if total_duration > 0 else 0

            logging.info(f"ğŸ“Š æ—¶é—´è½´è¦†ç›–æƒ…å†µ:")
            logging.info(f"   - å­—å¹•æ–‡ä»¶æ€»æ—¶é•¿: {total_duration_formatted}")
            logging.info(f"   - æ—¶é—´è½´æœ€åæ—¶é—´ç‚¹: {last_time_str}")
            logging.info(f"   - è¦†ç›–ç‡: {coverage_percentage:.1f}%")

            # æ›´ä¸¥æ ¼çš„è¦†ç›–ç‡æ£€æŸ¥
            if coverage_percentage < 70:
                logging.error(f"âŒ æ—¶é—´è½´è¦†ç›–ç‡è¿‡ä½ ({coverage_percentage:.1f}%)ï¼Œä¸¥é‡ä¸è¶³")
                logging.error(f"ğŸ’¡ æ—¶é—´è½´æœªèƒ½è¦†ç›–å­—å¹•æ–‡ä»¶çš„å¤§éƒ¨åˆ†å†…å®¹ï¼Œå»ºè®®é‡æ–°ç”Ÿæˆ")
            elif coverage_percentage < 85:
                logging.warning(f"âš ï¸ æ—¶é—´è½´è¦†ç›–ç‡åä½ ({coverage_percentage:.1f}%)ï¼Œå¯èƒ½é—æ¼é‡è¦å†…å®¹")
                logging.warning(f"ğŸ’¡ å»ºè®®æ£€æŸ¥ç”Ÿæˆçš„æ—¶é—´è½´æ˜¯å¦åŒ…å«å­—å¹•æ–‡ä»¶ç»“å°¾éƒ¨åˆ†çš„å†…å®¹")
            elif coverage_percentage >= 90:
                logging.info(f"âœ… æ—¶é—´è½´è¦†ç›–ç‡ä¼˜ç§€ ({coverage_percentage:.1f}%)")
            else:
                logging.info(f"âœ… æ—¶é—´è½´è¦†ç›–ç‡è‰¯å¥½ ({coverage_percentage:.1f}%)")

        except (ValueError, IndexError) as e:
            logging.warning(f"âš ï¸ æ—¶é—´è½´éªŒè¯å¤±è´¥: {e}")
            logging.warning(f"   æœ€åæ—¶é—´ç‚¹: {last_time_str}")

    def _create_professional_shownotes_prompt(self, segments: List[Dict[str, Any]], total_duration: float, total_duration_formatted: str) -> str:
        """åˆ›å»ºä¸“ä¸šçº§Show Notesç”Ÿæˆæç¤ºè¯ï¼ŒåŸºäºAcquiredæ’­å®¢é£æ ¼"""

        # ä½¿ç”¨å®Œæ•´çš„segmentsæ•°æ®ï¼Œä¸è¿›è¡Œå–æ ·
        total_segments = len(segments)

        logging.info(f"ğŸ“Š å°†å‘AIæä¾›å®Œæ•´çš„å­—å¹•æ–‡ä»¶ï¼š{total_segments} ä¸ªå­—å¹•æ®µè½ç”¨äºåˆ†æ")

        # å°†å®Œæ•´çš„segmentsè½¬æ¢ä¸ºJSONæ ¼å¼å­—ç¬¦ä¸²ï¼Œç”¨äºæç¤ºè¯
        segments_json = json.dumps(segments, ensure_ascii=False, indent=2)

        return f"""# === è§’è‰²ä¸æ ¸å¿ƒç›®æ ‡å®šä¹‰ ===
ä½ æ˜¯ä¸€ä½é¡¶å°–çš„å•†ä¸šä¸ç§‘æŠ€æ’­å®¢åˆ¶ä½œäººå…¼æ’°ç¨¿äººï¼Œä¸ºä¸€æ¡£é£æ ¼ç±»ä¼¼äºã€ŠAcquiredã€‹çš„ä¸­æ–‡æ’­å®¢ã€Šä»–å±±ä¹‹å£°ã€‹å·¥ä½œã€‚ã€Šä»–å±±ä¹‹å£°ã€‹æ˜¯ä¸€æ¡£ä¸“é—¨ç¼–è¯‘å…¨çƒä¼˜ç§€å¤–è¯­æ’­å®¢å†…å®¹çš„èŠ‚ç›®ï¼Œè‡´åŠ›äºä¸ºä¸­æ–‡å¬ä¼—å¸¦æ¥å…¨çƒæœ€å‰æ²¿çš„æ€è€ƒå’Œå¯¹è¯ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯åˆ†ææ‰€æä¾›çš„JSONæ ¼å¼æ’­å®¢å­—å¹•æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆä¸€å¥—å…¨é¢ã€å¯Œæœ‰æ´å¯ŸåŠ›ä¸”ç”±å™äº‹é©±åŠ¨çš„æ’­å®¢è¥é”€å†…å®¹ï¼ŒåŒ…æ‹¬é’©å­ã€å†…å®¹æ¦‚è¦ã€æ ‡é¢˜å€™é€‰å’Œä¸“ä¸šçº§Show Notesï¼ˆå«è¯¦ç»†çš„å¸¦æ—¶é—´æˆ³æ—¶é—´è½´ï¼‰ã€‚

# === å­—å¹•æ–‡ä»¶ä¿¡æ¯ ===
**å­—å¹•æ–‡ä»¶æ—¶é—´èŒƒå›´**: 00:00 - {total_duration_formatted} (æ€»è®¡{total_duration:.1f}ç§’)
**æä¾›çš„å­—å¹•æ®µè½æ•°**: {total_segments} (å®Œæ•´å­—å¹•æ–‡ä»¶)
**é‡è¦**: ä½ å¿…é¡»ç¡®ä¿ç”Ÿæˆçš„æ—¶é—´è½´è¦†ç›–ä»00:00åˆ°æ¥è¿‘{total_duration_formatted}çš„å®Œæ•´æ—¶é—´èŒƒå›´

# === è¾“å…¥æ•°æ®ç»“æ„å®šä¹‰ ===
è¾“å…¥æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼ŒåŒ…å«å­—å¹•ç‰‡æ®µåˆ—è¡¨ã€‚æ¯ä¸ªç‰‡æ®µåŒ…å«ï¼š"text"ï¼ˆå­—ç¬¦ä¸²ï¼Œæ–‡æœ¬å†…å®¹ï¼‰ã€"start"ï¼ˆæµ®ç‚¹æ•°ï¼Œå¼€å§‹æ—¶é—´ç§’ï¼‰ã€"end"ï¼ˆæµ®ç‚¹æ•°ï¼Œç»“æŸæ—¶é—´ç§’ï¼‰å’Œ"timestamp"ï¼ˆå­—ç¬¦ä¸²ï¼Œæ ¼å¼åŒ–çš„æ—¶é—´æˆ³ï¼‰ã€‚

# === é’©å­ç”Ÿæˆçš„å…³é”®è¦æ±‚ ===
**é‡è¦ï¼šé’©å­å¿…é¡»å®Œå…¨å¼•ç”¨åŸå§‹JSONå­—å¹•æ–‡ä»¶ä¸­çš„åŸè¯**
- é’©å­çš„"text"å­—æ®µå¿…é¡»æ˜¯ä»JSONå­—å¹•ç‰‡æ®µä¸­å®Œå…¨å¤åˆ¶çš„åŸå§‹æ–‡æœ¬ï¼Œä¸å…è®¸ä»»ä½•æ”¹å†™ã€é‡ç»„æˆ–ä¿®æ”¹
- å¯ä»¥é€‰æ‹©å•ä¸ªå®Œæ•´çš„å­—å¹•ç‰‡æ®µï¼Œæˆ–è€…é€‰æ‹©è¿ç»­çš„å¤šä¸ªå­—å¹•ç‰‡æ®µè¿›è¡Œç»„åˆ
- ä½†ç»å¯¹ä¸èƒ½å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œä»»ä½•ç¼–è¾‘ã€æ”¹å†™æˆ–é‡æ–°è¡¨è¿°
- æ—¶é—´æˆ³å¿…é¡»å‡†ç¡®å¯¹åº”æ‰€é€‰æ‹©çš„å­—å¹•ç‰‡æ®µçš„å®é™…æ—¶é—´æˆ³
- é€‰æ‹©æ ‡å‡†ï¼šåŸè¯å¿…é¡»å¯¹è§‚ä¼—æå…·å¸å¼•åŠ›ï¼Œèƒ½å¤Ÿæ¿€å‘å¥½å¥‡å¿ƒæˆ–æä¾›éœ‡æ’¼æ€§æ´å¯Ÿ

# === Show Notesè¯­æ°”å’Œå®šä½è¦æ±‚ ===
**é‡è¦ï¼šã€Šä»–å±±ä¹‹å£°ã€‹èŠ‚ç›®å®šä½å’Œè¯­æ°”è¦æ±‚**
- ã€Šä»–å±±ä¹‹å£°ã€‹æ˜¯ä¸€æ¡£ç¼–è¯‘èŠ‚ç›®ï¼Œæ‰€æœ‰å†…å®¹éƒ½æ¥è‡ªå…¶ä»–æ’­å®¢çš„ç¼–è¯‘å’Œè½¬è¿°
- èŠ‚ç›®ç®€ä»‹å¿…é¡»ä½“ç°ç¼–è¯‘æ€§è´¨ï¼Œä½¿ç”¨å®¢è§‚è½¬è¿°çš„è¯­æ°”ï¼Œé¿å…ç¬¬ä¸€äººç§°è¡¨è¿°
- æ­£ç¡®ç¤ºä¾‹ï¼š"æœ¬æœŸã€Šä»–å±±ä¹‹å£°ã€‹ä¸ºä½ å¸¦æ¥ã€ŠBig Technology Podcastã€‹çš„ä¸€æ¡£èŠ‚ç›®ï¼Œä¸Spyglassè®°è€…Mg Glerumä¸€èµ·ï¼Œæ·±åº¦æ‹†è§£ç§‘æŠ€å·¨å¤´åœ¨AIæ—¶ä»£çš„æˆ˜ç•¥ã€æƒè°‹ä¸å¤±æ§ç¬é—´"
- é”™è¯¯ç¤ºä¾‹ï¼š"æœ¬æœŸã€Šä»–å±±ä¹‹å£°ã€‹ä¸Spyglassè®°è€…Mg Glerumä¸€èµ·ï¼Œæ·±åº¦æ‹†è§£ç§‘æŠ€å·¨å¤´åœ¨AIæ—¶ä»£çš„æˆ˜ç•¥ã€æƒè°‹ä¸å¤±æ§ç¬é—´"
- å˜‰å®¾ç®€ä»‹ä¹Ÿè¦ä¿æŒå®¢è§‚è½¬è¿°çš„è¯­æ°”ï¼Œè¯´æ˜è¿™æ˜¯æ¥è‡ªåŸæ’­å®¢çš„å˜‰å®¾
- Show Noteså†…å®¹è¦å»é™¤Markdownæ ¼å¼æ ‡è®°ï¼Œç¡®ä¿åœ¨å°å®‡å®™ç­‰å¹³å°èƒ½æ­£å¸¸æ˜¾ç¤º

# === é“¾å¼æ€è€ƒï¼ˆChain-of-Thoughtï¼‰å·¥ä½œæµæŒ‡ä»¤ ===
ä¸ºäº†å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼Œä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹åˆ†æ­¥æµç¨‹ã€‚åœ¨å®Œæˆæ‰€æœ‰å†…éƒ¨æ€è€ƒæ­¥éª¤ä¹‹å‰ï¼Œä¸è¦è¾“å‡ºæœ€ç»ˆç»“æœã€‚è®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥æ€è€ƒï¼š

**ç¬¬ä¸€æ­¥ï¼šæ•´åˆå­—å¹• (Consolidate Transcript)**
é€šè¯»æ•´ä¸ªJSONç‰‡æ®µåˆ—è¡¨ã€‚å°†è¿ç»­çš„ç›¸å…³ç‰‡æ®µåˆå¹¶æˆæ›´å¤§çš„æ®µè½ï¼Œç”Ÿæˆä¸€ä»½å¹²å‡€ã€å¯è¯»çš„å¯¹è¯è„šæœ¬ã€‚

**ç¬¬äºŒæ­¥ï¼šè¯†åˆ«ä¸»é¢˜è¾¹ç•Œ (Identify Thematic Boundaries)**
ä»”ç»†é˜…è¯»æ•´åˆåçš„å¯¹è¯è„šæœ¬ï¼Œè¯†åˆ«ä¸»è¦çš„å™äº‹æˆ–ä¸»é¢˜è½¬æ¢ç‚¹ã€‚æ ‡è®°ä»¥ä¸‹è¿¹è±¡çš„æ–°ç« èŠ‚è¾¹ç•Œï¼š
- ä¸»æŒäººæ˜ç¡®è½¬æ¢è¯é¢˜
- é‡è¦æ–°æ¦‚å¿µã€äººç‰©ã€å…¬å¸æˆ–äº‹ä»¶è¢«å¼•å…¥
- å™äº‹æ—¶é—´çº¿å‡ºç°æ˜¾è‘—è·³è·ƒ
- å¿«é€Ÿé—®ç­”åå‡ºç°é•¿è§£é‡Šæ€§ç‹¬ç™½

**ç¬¬ä¸‰æ­¥ï¼šèµ·è‰ç« èŠ‚å¤§çº² (Draft Segment Outline)**
æ ¹æ®è¯†åˆ«çš„è¾¹ç•Œï¼Œåˆ›å»ºç« èŠ‚å¤§çº²è‰ç¨¿ã€‚å¯¹æ¯ä¸ªç« èŠ‚åˆ—å‡ºèµ·å§‹æ—¶é—´ã€ç»“æŸæ—¶é—´ï¼Œå¹¶ç”¨ä¸è¶…è¿‡15ä¸ªè¯æ¦‚æ‹¬æ ¸å¿ƒä¸»é¢˜ã€‚
**é‡è¦ï¼šç¡®ä¿æ—¶é—´è½´è¦†ç›–å®Œæ•´å­—å¹•æ–‡ä»¶æ—¶é•¿**
- æ—¶é—´è½´å¿…é¡»ä»00:00å¼€å§‹ï¼Œä¸€ç›´è¦†ç›–åˆ°æ¥è¿‘å­—å¹•æ–‡ä»¶ç»“æŸæ—¶é—´{total_duration_formatted}
- æœ€åä¸€ä¸ªæ—¶é—´ç‚¹åº”è¯¥åœ¨{total_duration_formatted}å‰çš„æœ€åå‡ åˆ†é’Ÿå†…
- ä½ å·²ç»è·å¾—äº†å®Œæ•´çš„å­—å¹•æ–‡ä»¶å†…å®¹ï¼Œè¯·ç¡®ä¿æ—¶é—´è½´è¦†ç›–æ•´ä¸ªå¯¹è¯çš„å®Œæ•´å‘å±•è¿‡ç¨‹

**ç¬¬å››æ­¥ï¼šè¯†åˆ«æœ€å…·å¸å¼•åŠ›çš„åŸå§‹å¼•ç”¨ (Identify Most Compelling Original Quotes)**
ä»”ç»†å®¡æŸ¥JSONå­—å¹•ç‰‡æ®µï¼Œå¯»æ‰¾æœ€å…·å¸å¼•åŠ›çš„åŸå§‹è¯è¯­ä½œä¸ºé’©å­å€™é€‰ã€‚è¿™äº›è¯è¯­å¿…é¡»ï¼š
- å®Œå…¨æ¥è‡ªåŸå§‹å­—å¹•ç‰‡æ®µï¼Œä¸åšä»»ä½•ä¿®æ”¹
- å…·æœ‰å¼ºçƒˆçš„å¸å¼•åŠ›ã€äº‰è®®æ€§æˆ–æ´å¯Ÿæ€§
- èƒ½å¤Ÿæ¿€å‘å¬ä¼—çš„å¥½å¥‡å¿ƒ
- å¯ä»¥æ˜¯å•ä¸ªç‰‡æ®µæˆ–è¿ç»­çš„å¤šä¸ªç‰‡æ®µ

**ç¬¬äº”æ­¥ï¼šéªŒè¯æ—¶é—´è½´å®Œæ•´æ€§ (Verify Timeline Completeness)**
æ£€æŸ¥ç”Ÿæˆçš„æ—¶é—´è½´æ˜¯å¦æ»¡è¶³ä»¥ä¸‹è¦æ±‚ï¼š
- ä»00:00å¼€å§‹
- æœ€åä¸€ä¸ªæ—¶é—´ç‚¹æ¥è¿‘å­—å¹•æ–‡ä»¶ç»“æŸæ—¶é—´{total_duration_formatted}
- æ—¶é—´ç‚¹åˆ†å¸ƒåˆç†ï¼Œæ²¡æœ‰å¤§çš„æ—¶é—´ç©ºéš™
- ç¡®ä¿æ—¶é—´è½´è¦†ç›–ç‡è¾¾åˆ°90%ä»¥ä¸Šï¼ˆå³æœ€åæ—¶é—´ç‚¹åº”åœ¨{total_duration_formatted}çš„90%ä»¥åï¼‰
- åŸºäºå®Œæ•´çš„å­—å¹•æ–‡ä»¶å†…å®¹ï¼Œç¡®ä¿æ—¶é—´è½´åæ˜ å¯¹è¯çš„å®Œæ•´å‘å±•è„‰ç»œ

**ç¬¬å…­æ­¥ï¼šç”Ÿæˆè¥é”€å†…å®¹ (Generate Marketing Content)**
åŸºäºåˆ†æç”Ÿæˆé’©å­ã€æ¦‚è¦ã€æ ‡é¢˜å’ŒShow Notesï¼Œé£æ ¼è¦å¼•äººå…¥èƒœä¸”å¯Œæœ‰æ´å¯ŸåŠ›ã€‚é’©å­å¿…é¡»ä¸¥æ ¼éµå¾ªç¬¬å››æ­¥çš„è¦æ±‚ã€‚

# === å°‘æ ·æœ¬å­¦ä¹ èŒƒä¾‹ ===
**Show Notesæ—¶é—´è½´é£æ ¼èŒƒä¾‹ï¼š**

### èŒƒä¾‹ 1
(00:12:45) åæ™®æ‹‰å¾·å®¶æ—çš„å¾·å›½æ¸Šæºä¸ä¸€æ¬¡å®¿å‘½èˆ¬çš„è´­ä¹°
å®œå®¶çš„æ•…äº‹å¹¶éå§‹äºç‘å…¸ï¼Œè€Œæ˜¯å¾·å›½ã€‚ä¸»æŒäººè¯¦ç»†è®²è¿°äº†è‹±æ ¼ç“¦Â·åæ™®æ‹‰å¾·çš„ç¥–çˆ¶æ¯å¦‚ä½•å› ä¸ºå®¶åº­åå¯¹ä»–ä»¬çš„å©šå§»ï¼Œè€Œä»å¾·å›½ç§»æ°‘è‡³ç‘å…¸ä¸€ä¸ªè‰°è‹¦çš„å†œä¸šåŒºã€‚ä»–ä»¬é€šè¿‡ä¸€æœ¬æ‚å¿—å¹¿å‘Šï¼Œåœ¨æœªæ›¾äº²çœ¼è§è¿‡çš„æƒ…å†µä¸‹ä¹°ä¸‹äº†ä¸€ä¸ªå†œåœºã€‚è¿™æ¬¡è¿ç§»åœ¨å®¶æ—å†…éƒ¨ç§ä¸‹äº†ä¸€é¢—æ·±åˆ»çš„æ–‡åŒ–ç§å­ï¼šä¸€ç§æ‘†è„±å›°å¢ƒã€åˆ›é€ è´¢å¯Œçš„å¼ºçƒˆé©±åŠ¨åŠ›ã€‚

### èŒƒä¾‹ 2
(00:03:10) ç†è§£Metaå‰æ‰€æœªæœ‰çš„è§„æ¨¡
Benå’ŒDavidåœ¨å¼€ç¯‡ä¾¿é€šè¿‡æ•°æ®æ­ç¤ºäº†MetaæƒŠäººçš„ç”¨æˆ·è§„æ¨¡â€”â€”40äº¿æœˆæ´»è·ƒç”¨æˆ·ï¼Œæ¥è¿‘å…¨çƒäººå£çš„ä¸€åŠã€‚ä»–ä»¬å°†å…¶ä¸å†å²ä¸Šæœ€åºå¤§çš„å¸å›½å’Œæ”¿åºœè¿›è¡Œæ¯”è¾ƒï¼Œå¾—å‡ºçš„ç»“è®ºæ˜¯ï¼šæ²¡æœ‰ä»»ä½•ä¸€ä¸ªæœºæ„æ›¾è¿æ¥è¿‡å¦‚æ­¤é«˜æ¯”ä¾‹çš„å…¨çƒäººå£ã€‚

# === è¾“å…¥æ•°æ® ===
**å­—å¹•æ•°æ®JSONï¼š**
```json
{segments_json}
```

# === æœ€ç»ˆè¾“å‡ºæ ¼å¼è§„èŒƒ ===
ä½ çš„æœ€ç»ˆè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š

```json
{{
  "hooks": {{
    "option_1": {{
      "sentences": [
        {{
          "text": "å®Œå…¨æ¥è‡ªåŸå§‹JSONå­—å¹•ç‰‡æ®µçš„åŸè¯ï¼Œä¸å…è®¸ä»»ä½•ä¿®æ”¹",
          "timestamp": "MM:SS",
          "reason": "é€‰æ‹©è¿™æ®µåŸè¯çš„ç†ç”±ï¼ˆä¸ºä»€ä¹ˆè¿™æ®µåŸè¯å…·æœ‰å¸å¼•åŠ›ï¼‰"
        }}
      ],
      "explanation": "ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªç»„åˆçš„è¯´æ˜ï¼Œå¼ºè°ƒè¿™äº›æ˜¯å®Œå…¨æœªç»ä¿®æ”¹çš„åŸå§‹å¼•ç”¨"
    }},
    "option_2": {{ ... }},
    "option_3": {{ ... }}
  }},
  "content_overview": "æ¬¢è¿æ”¶å¬ã€Šä»–å±±ä¹‹å£°ã€‹ï¼Œä¸€æ¡£è‡´åŠ›äºæ‰“ç ´è¯­è¨€å£å’ï¼Œä¸ºä½ å¸¦æ¥å…¨çƒæœ€å‰æ²¿æ€è€ƒçš„æ’­å®¢èŠ‚ç›®ã€‚æˆ‘æ˜¯ä½ çš„ä¸»æ’­ï¼ŒæŸ¥å“¥æŸ¥å°”æ–¯ã€‚[ç»§ç»­150-200å­—çš„å†…å®¹æ¦‚è¦]",
  "title_candidates": [
    {{
      "title": "æ ‡é¢˜1",
      "reason": "é€‰æ‹©ç†ç”±"
    }}
  ],
  "show_notes": {{
    "episode_intro": "èŠ‚ç›®ç®€ä»‹ï¼ˆ100-150å­—ï¼‰ï¼Œå¿…é¡»ä½“ç°ç¼–è¯‘æ€§è´¨ï¼Œä½¿ç”¨'æœ¬æœŸã€Šä»–å±±ä¹‹å£°ã€‹ä¸ºä½ å¸¦æ¥ã€ŠåŸæ’­å®¢åç§°ã€‹çš„ä¸€æ¡£èŠ‚ç›®'çš„æ ¼å¼å¼€å¤´ï¼Œä¿æŒå®¢è§‚è½¬è¿°è¯­æ°”",
    "guest_intro": "å˜‰å®¾ä»‹ç»ï¼ˆ100-150å­—ï¼‰ï¼Œå®¢è§‚ä»‹ç»åŸæ’­å®¢çš„å˜‰å®¾ï¼Œé¿å…ç¬¬ä¸€äººç§°è¡¨è¿°",
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3", "è¦ç‚¹4", "è¦ç‚¹5"],
    "timeline": [
      {{
        "time": "00:00",
        "topic": "å¼•äººå…¥èƒœçš„ç« èŠ‚æ ‡é¢˜",
        "description": "å¯Œæœ‰æ´å¯ŸåŠ›çš„æ‘˜è¦ï¼Œè¯´æ˜å¬ä¼—èƒ½è·å¾—ä»€ä¹ˆå…·ä½“ä»·å€¼ã€æ´è§æˆ–è¡ŒåŠ¨æŒ‡å¯¼ï¼ˆ50-100å­—ï¼‰",
        "section": "ç¬¬ä¸€éƒ¨åˆ†ï¼šä¸»é¢˜åç§°ï¼ˆå¯é€‰ï¼‰"
      }},
      {{
        "time": "XX:XX",
        "topic": "ä¸­é—´ç« èŠ‚æ ‡é¢˜",
        "description": "ç»§ç»­æä¾›æœ‰ä»·å€¼çš„å†…å®¹æè¿°",
        "section": "ç¬¬äºŒéƒ¨åˆ†ï¼šä¸»é¢˜åç§°ï¼ˆå¯é€‰ï¼‰"
      }},
      {{
        "time": "æ¥è¿‘{total_duration_formatted}çš„æ—¶é—´ç‚¹",
        "topic": "ç»“å°¾ç« èŠ‚æ ‡é¢˜",
        "description": "ç¡®ä¿æ—¶é—´è½´è¦†ç›–åˆ°éŸ³é¢‘ç»“å°¾é™„è¿‘",
        "section": "æœ€åéƒ¨åˆ†ï¼šä¸»é¢˜åç§°ï¼ˆå¯é€‰ï¼‰"
      }}
    ],
    "additional_notes": "å…³äºåŸèŠ‚ç›®ï¼šæœ¬æœŸå†…å®¹ç¼–è¯‘è‡ªçŸ¥åå¤–è¯­æ’­å®¢èŠ‚ç›®...\\n\\næŠ€æœ¯è¯´æ˜ï¼šæœ¬èŠ‚ç›®ä½¿ç”¨AIæŠ€æœ¯å¯¹åŸæœ‰èŠ‚ç›®äººå£°è¿›è¡Œå…‹éš†å’Œè¯­éŸ³åˆæˆååˆ¶ä½œè€Œæˆï¼Œå¯èƒ½åœ¨æŸäº›è¯­å¥çš„å‘éŸ³å¹¶ä¸èƒ½åšåˆ°å®Œå…¨è‡ªç„¶ã€‚\\n\\nå¬å‹äº¤æµï¼šå¦‚æœæœ‰æƒ³è¦å¬çš„å…¶ä»–å¤–è¯­æ’­å®¢æˆ–è€…èŠ‚ç›®ï¼Œæ¬¢è¿è”ç³»å¾®ä¿¡ï¼šchagexqï¼Œè¿›å…¥å¬å‹ç¾¤ã€‚"
  }}
}}
```

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºï¼Œç¡®ä¿æ—¶é—´è½´éƒ¨åˆ†é‡‡ç”¨ç±»ä¼¼Acquiredæ’­å®¢çš„æ·±åº¦åˆ†æé£æ ¼ï¼Œæ¯ä¸ªæ—¶é—´ç‚¹éƒ½è¦æœ‰å¼•äººå…¥èƒœçš„æ ‡é¢˜å’Œå¯Œæœ‰æ´å¯ŸåŠ›çš„æè¿°ã€‚

**æœ€é‡è¦çš„æé†’ï¼šæ—¶é—´è½´å®Œæ•´æ€§è¦æ±‚**
- æ—¶é—´è½´å¿…é¡»ä»00:00å¼€å§‹ï¼Œè¦†ç›–åˆ°æ¥è¿‘å­—å¹•æ–‡ä»¶ç»“æŸæ—¶é—´{total_duration_formatted}
- æœ€åä¸€ä¸ªæ—¶é—´ç‚¹åº”è¯¥åœ¨{total_duration_formatted}å‰çš„æœ€åå‡ åˆ†é’Ÿå†…
- æ—¶é—´ç‚¹åˆ†å¸ƒè¦åˆç†ï¼Œé¿å…å¤§çš„æ—¶é—´ç©ºéš™
- ä½ å·²ç»è·å¾—äº†å®Œæ•´çš„å­—å¹•æ–‡ä»¶å†…å®¹ï¼Œè¯·ç¡®ä¿æ—¶é—´è½´åæ˜ æ•´ä¸ªå¯¹è¯çš„å®Œæ•´å‘å±•è¿‡ç¨‹
- è¦†ç›–ç‡å¿…é¡»è¾¾åˆ°90%ä»¥ä¸Šï¼Œå³æœ€åæ—¶é—´ç‚¹åº”åœ¨æ€»æ—¶é•¿çš„90%ä»¥å

**é’©å­ç”Ÿæˆè¦æ±‚**
- é’©å­çš„"text"å­—æ®µå¿…é¡»æ˜¯ä»æä¾›çš„JSONå­—å¹•ç‰‡æ®µä¸­å®Œå…¨å¤åˆ¶çš„åŸå§‹æ–‡æœ¬
- ç»å¯¹ä¸å…è®¸å¯¹åŸå§‹æ–‡æœ¬è¿›è¡Œä»»ä½•æ”¹å†™ã€é‡ç»„ã€ç¼–è¾‘æˆ–ä¿®æ”¹
- å¯ä»¥é€‰æ‹©å•ä¸ªå®Œæ•´ç‰‡æ®µæˆ–è¿ç»­çš„å¤šä¸ªç‰‡æ®µï¼Œä½†æ–‡æœ¬å†…å®¹å¿…é¡»ä¿æŒå®Œå…¨ä¸€è‡´
- æ—¶é—´æˆ³å¿…é¡»å‡†ç¡®å¯¹åº”æ‰€é€‰æ‹©ç‰‡æ®µçš„å®é™…æ—¶é—´æˆ³
- é€‰æ‹©çš„åŸè¯å¿…é¡»å¯¹è§‚ä¼—æå…·å¸å¼•åŠ›ï¼Œèƒ½å¤Ÿæ¿€å‘å¥½å¥‡å¿ƒæˆ–æä¾›éœ‡æ’¼æ€§æ´å¯Ÿ"""

    def _call_llm_for_content_generation(self, prompt: str) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨LLMç”Ÿæˆæ’­å®¢å†…å®¹"""
        payload: Dict[str, Any] = {
            "model": self.llm_config.model,
            "messages": [
                {"role": "system", "content": PODCAST_CONTENT_GENERATION_ROLE},
                {"role": "user", "content": prompt},
            ],
            "temperature": self.llm_config.temperature,
            "top_p": self.llm_config.top_p,
        }
        if self.llm_config.max_tokens is not None:
            payload["max_tokens"] = self.llm_config.max_tokens
        if self.llm_config.response_format:
            payload["response_format"] = {"type": self.llm_config.response_format}

        try:
            response = post_chat_completions(
                self.endpoint,
                self.llm_config.api_key,
                payload,
                timeout=self.llm_config.timeout,
            )
        except SystemExit as exc:  # translate_openai_cli raises SystemExit on HTTP errors
            logging.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {exc}")
            return None
        except Exception as exc:  # noqa: BLE001
            logging.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {exc}")
            return None

        logging.debug(f"LLMå“åº”ç»“æ„: {response}")

        usage = response.get("usage")
        if usage:
            token_counts = {
                "prompt": usage.get("prompt_tokens", 0),
                "completion_tokens": usage.get("completion_tokens", 0),
                "total": usage.get("total_tokens", 0),
            }
            self._update_token_counts(token_counts)
            logging.info(
                "  - Tokenä½¿ç”¨: %s è¾“å…¥ + %s è¾“å‡º = %s æ€»è®¡",
                token_counts["prompt"],
                token_counts["completion_tokens"],
                token_counts["total"],
            )
        else:
            logging.warning("  - æ— tokenä½¿ç”¨ä¿¡æ¯å¯ç”¨")

        choices = response.get("choices") or []
        if not choices:
            logging.error("âŒ æœªæ”¶åˆ°æœ‰æ•ˆå“åº” (choices ä¸ºç©º)")
            return None

        content = choices[0].get("message", {}).get("content", "")
        if not content:
            logging.error("âŒ æœªæ”¶åˆ°æœ‰æ•ˆå“åº”å†…å®¹")
            return None

        logging.debug(f"åŸå§‹å“åº”å†…å®¹: {content[:500]}...")

        if "```json" in content:
            json_start = content.find("```json") + 7
            json_end = content.find("```", json_start)
            if json_end != -1:
                content = content[json_start:json_end].strip()
                logging.debug(f"æå–çš„JSONå†…å®¹: {content[:200]}...")

        try:
            return json.loads(content)
        except json.JSONDecodeError as exc:
            logging.error(f"âŒ JSONè§£æå¤±è´¥: {exc}")
            logging.error(f"å°è¯•è§£æçš„å†…å®¹: {content[:1000]}")
            return None

    def _create_comprehensive_content_with_plain_show_notes(self, podcast_content: Dict[str, Any]) -> str:
        """åˆ›å»ºåŒ…å«çº¯æ–‡æœ¬Show Notesçš„ç»¼åˆå†…å®¹æ–‡æ¡£"""
        if not podcast_content:
            return "# æ’­å®¢å†…å®¹ç”Ÿæˆå¤±è´¥\n\næ— æ³•ç”Ÿæˆæ’­å®¢å†…å®¹ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®ã€‚"

        content = []
        content.append("# ã€Šä»–å±±ä¹‹å£°ã€‹æ’­å®¢å†…å®¹åŒ…")
        content.append("")
        content.append("æœ¬æ–‡æ¡£åŒ…å«æœ¬æœŸæ’­å®¢çš„å®Œæ•´è¥é”€å†…å®¹ï¼ŒåŒ…æ‹¬é’©å­ã€å†…å®¹æ¦‚è¦ã€æ ‡é¢˜å€™é€‰å’ŒShow Notesã€‚")
        content.append("")
        content.append("---")
        content.append("")

        # 1. æ’­å®¢é’©å­
        content.append("## 1ï¸âƒ£ æ’­å®¢é’©å­ (Hooks)")
        content.append("")
        content.append("**ä½¿ç”¨è¯´æ˜ï¼š** ä»¥ä¸‹æ˜¯3ç»„é’©å­ç»„åˆï¼Œå¯æ ¹æ®ç›®æ ‡å—ä¼—å’Œå¹³å°ç‰¹ç‚¹é€‰æ‹©ä½¿ç”¨ã€‚")
        content.append("")

        hooks = podcast_content.get("hooks", {})
        for i, (_, hook_data) in enumerate(hooks.items(), 1):
            content.append(f"### ç»„åˆ {i}")
            content.append("")

            sentences = hook_data.get("sentences", [])
            for sentence_data in sentences:
                if isinstance(sentence_data, dict):
                    text = sentence_data.get("text", "")
                    timestamp = sentence_data.get("timestamp", "")
                    reason = sentence_data.get("reason", "")

                    if timestamp:
                        content.append(f"- **[{timestamp}]** {text}")
                    else:
                        content.append(f"- {text}")

                    if reason:
                        content.append(f"  *é€‰æ‹©ç†ç”±ï¼š{reason}*")
                else:
                    content.append(f"- {sentence_data}")
                content.append("")

            explanation = hook_data.get("explanation", "")
            if explanation:
                content.append(f"**ä¸ºä»€ä¹ˆè¿™ä¸ªç»„åˆæœ‰æ•ˆï¼š** {explanation}")
                content.append("")

        content.append("---")
        content.append("")

        # 2. å†…å®¹æ¦‚è¦
        content.append("## 2ï¸âƒ£ å†…å®¹æ¦‚è¦ (Content Overview)")
        content.append("")
        content.append("**ä½¿ç”¨è¯´æ˜ï¼š** æ­¤å†…å®¹æ¦‚è¦æ”¾åœ¨é’©å­ä¹‹åã€æ­£å¼æ’­å®¢å†…å®¹ä¹‹å‰ã€‚")
        content.append("")

        overview = podcast_content.get("content_overview", "")
        if overview:
            content.append(overview)
        else:
            content.append("å†…å®¹æ¦‚è¦ç”Ÿæˆå¤±è´¥ã€‚")

        content.append("")
        content.append("---")
        content.append("")

        # 3. æ ‡é¢˜å€™é€‰
        content.append("## 3ï¸âƒ£ æ ‡é¢˜å€™é€‰ (Title Candidates)")
        content.append("")
        content.append("**ä½¿ç”¨è¯´æ˜ï¼š** ä»¥ä¸‹æ˜¯5ä¸ªæ ‡é¢˜å€™é€‰ï¼Œå¯æ ¹æ®å¹³å°ç‰¹ç‚¹å’Œç›®æ ‡å—ä¼—é€‰æ‹©ä½¿ç”¨ã€‚")
        content.append("")

        titles = podcast_content.get("title_candidates", [])
        for i, title_data in enumerate(titles, 1):
            if isinstance(title_data, dict):
                title = title_data.get("title", "")
                reason = title_data.get("reason", "")

                content.append(f"### æ ‡é¢˜ {i}")
                content.append("")
                content.append(f"**{title}**")
                content.append("")
                if reason:
                    content.append(f"**é€‰æ‹©ç†ç”±ï¼š** {reason}")
                content.append("")
            else:
                content.append(f"### æ ‡é¢˜ {i}")
                content.append("")
                content.append(f"**{title_data}**")
                content.append("")

        content.append("---")
        content.append("")

        # 4. Show Notes (çº¯æ–‡æœ¬ç‰ˆæœ¬)
        content.append("## 4ï¸âƒ£ Show Notesï¼ˆçº¯æ–‡æœ¬ç‰ˆæœ¬ï¼Œé€‚ç”¨äºå°å®‡å®™ç­‰å¹³å°ï¼‰")
        content.append("")
        content.append("**ä½¿ç”¨è¯´æ˜ï¼š** ä»¥ä¸‹Show Noteså·²å»é™¤Markdownæ ¼å¼ï¼Œå¯ç›´æ¥å¤åˆ¶ç²˜è´´åˆ°å°å®‡å®™ç­‰æ’­å®¢å¹³å°ã€‚")
        content.append("")
        content.append("```")

        show_notes = podcast_content.get("show_notes", {})

        # èŠ‚ç›®ç®€ä»‹
        episode_intro = show_notes.get("episode_intro", "")
        if episode_intro:
            content.append("ğŸ™ï¸ æœ¬æœŸèŠ‚ç›®ç®€ä»‹")
            content.append("")
            content.append(episode_intro)
            content.append("")

        # å˜‰å®¾ä»‹ç»
        guest_intro = show_notes.get("guest_intro", "")
        if guest_intro:
            content.append("ğŸ§‘â€ğŸ’» æœ¬æœŸå˜‰å®¾ç®€ä»‹")
            content.append("")
            content.append(guest_intro)
            content.append("")

        # æ ¸å¿ƒè¦ç‚¹
        key_points = show_notes.get("key_points", [])
        if key_points:
            content.append("ğŸ”‘ æœ¬æœŸæ ¸å¿ƒè¦ç‚¹")
            content.append("")
            for point in key_points:
                content.append(f"â€¢ {point}")
            content.append("")

        # æ—¶é—´è½´
        timeline = show_notes.get("timeline", [])
        if timeline:
            content.append("â° æ—¶é—´è½´")
            content.append("")
            for item in timeline:
                if isinstance(item, dict):
                    time = item.get("time", "")
                    topic = item.get("topic", "")
                    description = item.get("description", "")

                    content.append(f"{time} {topic}")
                    if description:
                        content.append(description)
                    content.append("")

        # è¡¥å……è¯´æ˜
        additional_notes = show_notes.get("additional_notes", "")
        if additional_notes:
            content.append("ğŸ“ è¡¥å……è¯´æ˜")
            content.append("")
            content.append(additional_notes)
            content.append("")

        content.append("```")
        content.append("")
        content.append("---")
        content.append("")
        content.append("*æœ¬å†…å®¹ç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œè¯·æ ¹æ®å®é™…éœ€è¦è¿›è¡Œè°ƒæ•´å’Œä¼˜åŒ–ã€‚*")

        return "\n".join(content)


def main() -> int:
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç‹¬ç«‹æ’­å®¢å†…å®¹ç”Ÿæˆå™¨")
    parser.add_argument("subtitle_file", type=Path, help="JSONå­—å¹•æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "-o",
        "--output",
        type=Path,
        default=None,
        help="è¾“å‡ºç›®å½• (é»˜è®¤: ä¸å­—å¹•æ–‡ä»¶åŒè·¯å¾„ä¸‹çš„ podcast_marketing_outputs)",
    )
    parser.add_argument(
        "--env",
        type=Path,
        default=DEFAULT_ENV_PATH,
        help="OpenAI å…¼å®¹æœåŠ¡é…ç½® env æ–‡ä»¶ (é»˜è®¤: tools/openai_translator.env)",
    )
    parser.add_argument("--base-url", help="è¦†ç›– env ä¸­çš„ OPENAI_BASE_URL")
    parser.add_argument("--api-key", help="è¦†ç›– env ä¸­çš„ OPENAI_API_KEY")
    parser.add_argument("--model", help="è¦†ç›– env ä¸­çš„ OPENAI_MODEL")
    parser.add_argument("--temperature", type=float, default=0.2)
    parser.add_argument("--top-p", type=float, default=1.0)
    parser.add_argument("--max-tokens", type=int)
    parser.add_argument("--timeout", type=int, default=300, help="HTTP è¶…æ—¶ï¼ˆç§’ï¼‰")
    parser.add_argument(
        "--response-format",
        default="json_object",
        help="OpenAI response_format.typeï¼Œå¡« none å…³é—­",
    )
    parser.add_argument("-v", "--verbose", action="store_true", help="è¯¦ç»†æ—¥å¿—è¾“å‡º")

    args = parser.parse_args()

    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s [%(levelname)s] - %(message)s",
        handlers=[logging.StreamHandler()],
    )

    subtitle_path = args.subtitle_file.expanduser().resolve()
    if not subtitle_path.is_file():
        logging.error(f"âŒ å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {subtitle_path}")
        return 1

    if args.output is not None:
        output_dir = args.output.expanduser().resolve()
    else:
        output_dir = (subtitle_path.parent / "podcast_marketing_outputs").resolve()
    env_path = args.env.expanduser().resolve()

    env_vars = load_env_file(env_path)

    base_url = args.base_url or env_vars.get("OPENAI_BASE_URL") or "https://api.openai.com"
    api_key = args.api_key or env_vars.get("OPENAI_API_KEY") or ""
    model = args.model or env_vars.get("OPENAI_MODEL") or "gpt-4o-mini"

    response_format = args.response_format
    if response_format:
        if str(response_format).lower() in {"none", "null", "off"}:
            response_format_value: Optional[str] = None
        else:
            response_format_value = str(response_format)
    else:
        response_format_value = None

    if not api_key:
        logging.warning("OPENAI_API_KEY æœªé…ç½®ï¼ˆenv æˆ– --api-keyï¼‰ã€‚è‹¥ç›®æ ‡æœåŠ¡éœ€è¦é‰´æƒï¼Œè¯·è®¾ç½®ã€‚")

    llm_config = LLMConfig(
        base_url=base_url,
        api_key=api_key,
        model=model,
        temperature=args.temperature,
        top_p=args.top_p,
        max_tokens=args.max_tokens,
        timeout=args.timeout,
        response_format=response_format_value,
    )

    try:
        generator = EnhancedPodcastContentGenerator(llm_config)
        output_file = generator.generate_content_from_subtitle(subtitle_path, output_dir)

        if output_file:
            print("\nâœ… æ’­å®¢å†…å®¹ç”Ÿæˆå®Œæˆï¼")
            print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
            return 0

        print("\nâŒ æ’­å®¢å†…å®¹ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯")
        return 1
    except Exception as exc:  # noqa: BLE001
        logging.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {exc}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
